{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MLE101","text":"<p>This is a place presenting questions list/checklist and learning resources for MLE interview. </p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>Coding: Algorithms and coding</li> <li>ML Coding: Classic ML implementations questions</li> <li>ML Q&amp;A: Frequently asked ML, DL, and GenAI questions</li> <li>ML System Design: High-level ML design problems</li> </ul>"},{"location":"#connect","title":"\ud83d\udd17 Connect","text":"<ul> <li>GitHub Repo: MLE101 on GitHub</li> <li>Author: \u5927\u53ea\u5927\u53ea\u55b5 (\u5c0f\u7ea2\u4e66 / RedNote)</li> </ul>"},{"location":"coding/","title":"Coding","text":""},{"location":"coding/#checklist","title":"Checklist","text":""},{"location":"coding/#array-string","title":"Array &amp; String","text":"<ul> <li>Two Pointers</li> <li>Sliding Window</li> <li>Prefix Sum</li> <li>Binary Search (upper limit, lower limit)</li> </ul>"},{"location":"coding/#hashing","title":"Hashing","text":"<ul> <li>HashMap</li> <li>HashSet</li> <li>Frequency Counter</li> </ul>"},{"location":"coding/#stack-queue","title":"Stack &amp; Queue","text":"<ul> <li>Stack / Monotonic Stack</li> <li>Queue / Deque</li> <li>Min/Max Stack</li> </ul>"},{"location":"coding/#linked-list","title":"Linked List","text":"<ul> <li>Fast and Slow Pointers</li> <li>Reverse Linked List</li> <li>Detect Cycle</li> </ul>"},{"location":"coding/#tree","title":"Tree","text":"<ul> <li>DFS (Preorder / Inorder / Postorder)</li> <li>BFS (Level Order)</li> <li>Recursive Tree Traversal</li> <li>Binary Search Tree </li> </ul>"},{"location":"coding/#graph","title":"Graph","text":"<ul> <li>BFS</li> <li>DFS</li> <li>Union Find / Disjoint Set</li> <li>Topological Sort (Kahn\u2019s Algorithm, DFS)</li> <li>Dijkstra's Algorithm</li> </ul>"},{"location":"coding/#recursion-backtracking","title":"Recursion &amp; Backtracking","text":"<ul> <li>Subsets / Permutations / Combinations</li> <li>N-Queens</li> <li>Sudoku Solver</li> <li>Word Search</li> </ul>"},{"location":"coding/#heap-priority-queue","title":"Heap &amp; Priority Queue","text":"<ul> <li>Min Heap / Max Heap</li> <li>Kth Largest / Smallest</li> <li>Median Finder</li> </ul>"},{"location":"coding/#dynamic-programming","title":"Dynamic Programming","text":"<ul> <li>Longest Increasing Subsequence</li> <li>Longest Common Subsequence</li> <li>DP on Grid</li> <li>Memoization + Tabulation</li> </ul>"},{"location":"coding/#greedy","title":"Greedy","text":"<ul> <li>Interval Scheduling</li> <li>Jump Game</li> </ul>"},{"location":"coding/#learning-resources","title":"Learning Resources","text":""},{"location":"coding/#english","title":"English","text":"<ul> <li>NeetCode</li> <li>Labuladong Algo Notes</li> <li>Meta Top 200 leetcode questions</li> </ul>"},{"location":"coding/#chinese","title":"Chinese","text":"<ul> <li>\u4ee3\u7801\u968f\u60f3\u5f55</li> </ul>"},{"location":"ml_coding/","title":"ML Coding","text":""},{"location":"ml_coding/#ml-coding-checklist","title":"ML Coding Checklist","text":""},{"location":"ml_coding/#classic-ml-algorithms","title":"Classic ML Algorithms","text":"<ul> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Decision Tree (Information Gain Calculation)</li> <li>Random Forest (Voting)</li> <li>K-Means Clustering</li> <li>K-Nearest Neighbors (KNN)</li> </ul>"},{"location":"ml_coding/#optimization","title":"Optimization","text":"<ul> <li>Gradient Descent (with/without regularization)</li> <li>Cross-entropy Loss</li> </ul>"},{"location":"ml_coding/#deep-learning-foundations","title":"Deep Learning Foundations","text":"<ul> <li>Convolutional Neural Network (CNN)</li> <li>Convolution Layer</li> <li>Pooling Layer (Max, Average)</li> <li>Graph Neural Network (GNN)</li> <li>Message Passing Mechanism</li> <li>Transformer</li> <li>Query / Key / Value Computation</li> <li>Scaled Dot-Product Attention</li> <li>Positional Encoding</li> <li>Layer Normalization</li> </ul>"},{"location":"ml_coding/#learning-resources","title":"Learning Resources","text":"<ul> <li>alirezadir / ML Coding Questions </li> <li>Transformers: From Zero to Hero (YouTube)</li> <li>ChatGPT: Ask ChatGPT to walk through implementation details of any ML algorithm from scratch.</li> </ul>"},{"location":"ml_qa/","title":"ML Q&amp;A","text":""},{"location":"ml_qa/#fundamental-concepts","title":"Fundamental Concepts","text":"<ul> <li>What is bias vs variance?</li> <li>What is overfitting vs underfitting?</li> <li>How does regularization help prevent overfitting?</li> <li>What is the difference between L1 and L2 regularization?</li> <li>What is the difference between generative and discriminative models?</li> <li>What is the difference between supervised and unsupervised learning?</li> <li>What is the difference between LDA and PCA?</li> <li>What is the curse of dimensionality?</li> </ul>"},{"location":"ml_qa/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>What are precision, recall, and F1 score?</li> <li>What is an ROC curve, and how is it used?</li> <li>What is AUC and how is it interpreted?</li> <li>What is log loss / cross-entropy loss?</li> <li>What are confusion matrices and how do you interpret them?</li> <li>What is the difference between micro, macro, and weighted averaging in classification metrics?</li> <li>How do you evaluate a model for multi-class vs multi-label classification?</li> <li>What metrics are suitable for ranking models? </li> </ul>"},{"location":"ml_qa/#optimization","title":"Optimization","text":"<ul> <li>How does gradient descent work?</li> <li>What is stochastic gradient descent, and how is it different from full-batch gradient descent?</li> <li>What is backward propagation, and how does it relate to neural networks?</li> <li>What are common variants of gradient descent (SGD, Momentum, Adam, RMSProp)?</li> <li>What are vanishing and exploding gradients?</li> <li>How does learning rate affect training?</li> <li>What is gradient clipping and why is it used?</li> <li>How do optimizers differ in convergence speed and stability?</li> </ul>"},{"location":"ml_qa/#ml-algorithms","title":"ML Algorithms","text":"<ul> <li>What is logistic regression? (forward calculation, loss function)</li> <li>What is K-Nearest Neighbors (KNN)?</li> <li>What is decision tree learning?</li> <li>What is random forest, and how does it differ from gradient boosting?</li> <li>What is the difference between bagging and boosting?</li> <li>What is K-Means clustering, and how does it work?</li> <li>What is Support Vector Machine (SVM)?</li> <li>What is Bayesian learning?</li> <li>What is the difference between MAP and MLE?</li> <li>What is Naive Bayes and when is it effective?</li> <li>What are the assumptions behind linear regression?</li> <li>How does ridge regression differ from lasso regression?</li> <li>What are the strengths and weaknesses of tree-based methods?</li> <li>What are ensemble methods, and why do they work?</li> </ul>"},{"location":"ml_qa/#data-issues","title":"Data Issues","text":"<ul> <li>How does imbalanced data affect model performance?</li> <li>What are common strategies to handle imbalanced data?</li> <li>What is data drift and how do you handle it?</li> <li>How to handle missing data in ML pipelines?</li> <li>How do you split your data into train/validation/test?</li> <li>What are the benefits and risks of oversampling and undersampling?</li> </ul>"},{"location":"ml_qa/#deep-learning","title":"Deep Learning","text":"<ul> <li>What is the ReLU activation function?</li> <li>How to deal with gradient vanishing?</li> <li>What are common activation functions and when to use them (ReLU, sigmoid, tanh, GELU, etc.)?</li> <li>What are fully connected layers?</li> <li>What is the role of initialization in deep learning?</li> <li>What is dropout and how does it prevent overfitting?</li> <li>What is an epoch, batch, and iteration?</li> <li>What is early stopping and how does it help generalization?</li> <li>What is the difference between feedforward networks and recurrent networks?</li> </ul>"},{"location":"ml_qa/#transformers","title":"Transformers","text":"<ul> <li>Why do we divide the attention score by \u221ad\u2096 in the Transformer?</li> <li>Why are different weight matrices used to compute Q, K, and V?</li> <li>Why do we use Multi-Head Attention?</li> <li>What is the time complexity of attention?</li> <li>What is KV cache and why is it used?</li> <li>What is Multi-Query Attention (MQA)?</li> <li>What is Grouped Query Attention (GQA)?</li> <li>What are the shapes of Q, K, V in MHA, MQA, and GQA?</li> <li>What is Flash Attention and how does it work?</li> <li>How to optimize memory usage in attention mechanisms?</li> </ul>"},{"location":"ml_qa/#llm-architecture-inference","title":"LLM Architecture &amp; Inference","text":"<ul> <li>What are the key factors that affect LLM inference latency?</li> <li>What are common LLM inference optimization techniques?</li> <li>What is KV cache and how does it improve inference?</li> <li>What is smart batching and how does it affect performance?</li> <li>What is quantization and how does it help inference?</li> <li>What is the tradeoff in using MQA or GQA?</li> </ul>"},{"location":"ml_qa/#llm-fine-tuning-techniques","title":"LLM Fine-Tuning Techniques","text":"<ul> <li>What are typical fine-tuning methods for LLMs?</li> <li>What is LoRA and how does it work?</li> <li>How is the loss calculated in supervised fine-tuning (SFT)?</li> <li>What is the difference between prefix tuning and prompt tuning?</li> <li>What is RLHF and how does it work?</li> </ul>"},{"location":"ml_qa/#llm-training-optimization","title":"LLM Training Optimization","text":"<ul> <li>What is mixed-precision training and how does it work?</li> <li>What are the benefits and tradeoffs of FP16 vs BF16?</li> <li>What is gradient checkpointing?</li> <li>What is Distributed Data Parallel (DDP)?</li> <li>What is Fully Sharded Data Parallel (FSDP)?</li> <li>What is ZeRO (Zero Redundancy Optimizer) and its stages?</li> <li>How does ZeRO enable large-scale model training?</li> </ul>"},{"location":"ml_qa/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"<ul> <li>Why use RAG for LLMs?</li> <li>How does RAG architecture work?</li> <li>What are the steps to build a RAG-based chatbot?</li> </ul>"},{"location":"ml_qa/#learning-resources","title":"Learning Resources","text":""},{"location":"ml_qa/#github-repositories","title":"GitHub Repositories","text":"<ul> <li>andrewekhalel / MLQuestions   A curated list of ML interview questions categorized by topic, with some helpful explanations.</li> <li>khangich / machine-learning-interview   Interview prep notebook covering theory and code implementations for ML algorithms.</li> </ul>"},{"location":"ml_qa/#blogs-articles","title":"Blogs &amp; Articles","text":""},{"location":"ml_qa/#general-ml","title":"General ML","text":"<ul> <li>Multicollinearity in Regression Analysis \u2013 Statistics by Jim   Explains the impact of multicollinearity on model interpretability and stability.</li> <li>Feature Scaling Overview \u2013 Sebastian Raschka   Covers different scaling techniques and their effect on ML models.</li> <li>Gradient Boosting vs Random Forest   An intuitive side-by-side comparison with examples.</li> <li>L1 vs L2 Regularization (Visual Explanation)   Illustrates geometric differences and impact on sparsity.</li> <li>PCA Explained   A step-by-step walkthrough of PCA with visuals.</li> <li>Logistic Regression Detailed Overview   Covers math, intuition, and loss functions.</li> </ul>"},{"location":"ml_qa/#clustering","title":"Clustering","text":"<ul> <li>K-Means Intuition (Stanford CS221)   Visual and interactive explanation of the K-Means algorithm.</li> </ul>"},{"location":"ml_qa/#transformers-dl","title":"Transformers &amp; DL","text":"<ul> <li>Cross-Attention in Transformers   Cross Attention Explanation.</li> <li>BatchNorm vs LayerNorm   Explains when and why to use each normalization technique.</li> <li>Bagging vs Boosting   Good summary for tree-based ensemble learners.</li> </ul>"},{"location":"ml_qa/#research-papers","title":"Research Papers","text":"<ul> <li>Attention Is All You Need \u2013 Vaswani et al.   The original paper that introduced the Transformer architecture.</li> <li>CLIP: Learning Transferable Visual Models From Natural Language CLIP Paper (OpenAI) \u2013 Vision-language model combining image and text representations.</li> </ul>"},{"location":"ml_qa/#courses","title":"Courses","text":"<ul> <li> <p>Deep Learning Specialization (Coursera by Andrew Ng)   A five-course series covering neural networks, CNNs, RNNs, optimization, and structuring deep learning projects. Ideal for building strong foundational intuition.</p> </li> <li> <p>Generative AI with LLMs (DeepLearning.AI)   Covers modern LLM workflows, from prompt engineering to RAG and fine-tuning.</p> </li> </ul>"},{"location":"ml_system_design/","title":"ML System Design","text":""},{"location":"ml_system_design/#key-topics","title":"Key Topics","text":""},{"location":"ml_system_design/#ranking-retrieval-systems","title":"Ranking &amp; Retrieval Systems","text":"<ul> <li>Ads Ranking</li> <li>Search Engine Ranking</li> <li>Product / Video / Text / Image Recommendation</li> <li>Embedding Generation and Retrieval</li> <li>Personalized Feed Ranking</li> </ul>"},{"location":"ml_system_design/#knowledge-based-social-systems","title":"Knowledge-Based &amp; Social Systems","text":"<ul> <li>People You May Know</li> <li>Expertise or Creator Discovery</li> <li>Hashtag or Topic Suggestions</li> </ul>"},{"location":"ml_system_design/#generative-ai-systems","title":"Generative AI Systems","text":"<ul> <li>Chatbot with RAG (Retrieval-Augmented Generation)</li> <li>Generative Recommendation (e.g., Smart Replies)</li> </ul>"},{"location":"ml_system_design/#trust-safety-and-moderation","title":"Trust, Safety, and Moderation","text":"<ul> <li>Detect Unsafe Content</li> <li>Spam / Bot / Fake Account Detection</li> <li>Content Moderation Pipelines </li> </ul>"},{"location":"ml_system_design/#learning-resources","title":"Learning Resources","text":""},{"location":"ml_system_design/#interview-focused","title":"Interview-Focused","text":"<ul> <li> <p>ML System Design Interview (Educative.io)   A structured course covering common ML system design interview questions and patterns.</p> </li> <li> <p>Grokking the Machine Learning Interview (Educative.io)   Focuses on coding + system design for MLEs, with real-world case studies and Q&amp;A format.</p> </li> <li> <p>Machine Learning System Design Interview by Alex Xu </p> </li> <li> <p>Generative AI System Design Interview by Alex Xu </p> </li> <li> <p>Company's Tech Blog </p> </li> </ul>"}]}